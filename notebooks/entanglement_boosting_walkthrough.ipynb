{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entanglement Boosting: Step-by-Step Complete Understanding Guide\n",
    "\n",
    "This notebook walks through the entire repository step by step.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Setup and Imports** - Checking required libraries\n",
    "2. **Stim Basics** - Fundamentals of quantum circuit simulation library\n",
    "3. **Utility Layer Deep Dive** - QubitMapping, Circuit, NoiseConfiguration\n",
    "4. **Surface Code Understanding** - How syndrome measurements work\n",
    "5. **Entanglement Boosting Protocol** - Details of the main algorithm\n",
    "6. **Simulation and Result Analysis** - How to interpret results\n",
    "7. **Advanced Examples** - With noise and post-selection enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports\n",
    "\n",
    "If you haven't installed dependencies yet, run: `pip install -r requirements.txt`.\n",
    "\n",
    "This repository uses the following libraries:\n",
    "- **stim**: Quantum circuit simulation library\n",
    "- **pymatching**: Minimum weight matching decoder (for error correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import stim\n",
    "import pymatching\n",
    "\n",
    "# Make sure we can import local modules\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "import entanglement_boosting as eb\n",
    "import util\n",
    "import surface_code\n",
    "\n",
    "print('stim', stim.__version__)\n",
    "print('pymatching', pymatching.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stim Basics: Building a Tiny Circuit\n",
    "\n",
    "Stim is a high-performance quantum circuit simulation library. Let's start by creating a simple circuit that applies an H gate to a single qubit and measures it in the Z basis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = stim.Circuit()\n",
    "c.append('H', 0)\n",
    "c.append('M', 0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stim Basics: Sampling\n",
    "\n",
    "Use `compile_sampler()` to generate measurement samples. You can run the quantum circuit multiple times (shots) to obtain statistics on measurement results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = c.compile_sampler()\n",
    "samples = sampler.sample(shots=10)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Layer: Deep Dive into Circuit Wrapper\n",
    "\n",
    "This project uses a `util.Circuit` class that wraps `stim.Circuit`. This wrapper enforces the following constraints:\n",
    "\n",
    "1. **Nearest-neighbor only**: Two-qubit gates (CX) are only allowed between adjacent qubits\n",
    "2. **One gate per tick**: Each qubit can participate in at most one gate per tick\n",
    "3. **Automatic noise injection**: Automatically injects appropriate noise based on the gate\n",
    "\n",
    "### Understanding QubitMapping\n",
    "\n",
    "Qubits are arranged on a 2D grid, and the mapping manages the relationship between coordinates (x, y) and IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small mapping and a noiseless circuit\n",
    "mapping = util.QubitMapping(width=3, height=3)\n",
    "print(f\"Mapping size: width={mapping.width}, height={mapping.height}\")\n",
    "print(f\"Total number of qubits: {len(mapping.mapping)}\")\n",
    "print(f\"First 5 qubits: {mapping.mapping[:5]}\")\n",
    "\n",
    "# Example of getting ID from coordinates\n",
    "print(f\"\\nQubit ID at coordinate (0,0): {mapping.get_id(0, 0)}\")\n",
    "print(f\"Qubit ID at coordinate (1,1): {mapping.get_id(1, 1)}\")\n",
    "\n",
    "# Create circuit with noiseless configuration\n",
    "noise = util.NO_ERROR_CONF\n",
    "cwrap = util.Circuit(mapping, noise)\n",
    "\n",
    "# Place a few operations using the wrapper\n",
    "print(\"\\n=== Circuit Construction ===\")\n",
    "cwrap.place_reset_x((0, 0))  # Reset in X basis\n",
    "cwrap.place_reset_z((2, 0))  # Reset in Z basis\n",
    "cwrap.place_cx((0, 0), (1, 1))  # Controlled-NOT gate (nearest-neighbor only)\n",
    "cwrap.place_measurement_z((2, 0))  # Measure in Z basis\n",
    "cwrap.place_tick()  # Place tick (adds idle noise)\n",
    "\n",
    "print(\"\\nGenerated circuit:\")\n",
    "print(cwrap.circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Surface Code Patch Layout\n",
    "\n",
    "The entanglement boosting protocol builds two surface code patches and performs syndrome measurement rounds.\n",
    "\n",
    "### What is Surface Code?\n",
    "\n",
    "Surface code is an error-correcting code that uses qubits arranged on a 2D grid. Each patch has:\n",
    "- **Data qubits**: Hold logical quantum information\n",
    "- **Ancilla qubits**: Used for syndrome measurements\n",
    "\n",
    "Syndrome measurements detect errors by alternately measuring X and Z stabilizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a surface code patch\n",
    "mapping = util.QubitMapping(width=9, height=5)\n",
    "cwrap = util.Circuit(mapping, util.NO_ERROR_CONF)\n",
    "\n",
    "# SurfaceCodePatch parameters:\n",
    "# - offset: Starting position of the patch\n",
    "# - bell_distance: Distance of Bell pair (d_Bell)\n",
    "# - distance: Surface code distance (d_s)\n",
    "patch = eb.SurfaceCodePatch(cwrap, offset=(1, 1), bell_distance=2, distance=3)\n",
    "\n",
    "print(f\"Total number of syndrome measurements: {len(patch.syndrome_measurements)}\")\n",
    "print(f\"\\nFirst 5 syndrome measurement positions:\")\n",
    "for i, (pos, measurement) in enumerate(list(patch.syndrome_measurements.items())[:5]):\n",
    "    print(f\"  {i+1}. Position {pos}: {type(measurement).__name__}\")\n",
    "\n",
    "# Check logical X and Z Pauli strings\n",
    "print(f\"\\nLogical X Pauli string: {patch.logical_x_pauli_string()}\")\n",
    "print(f\"Logical Z Pauli string: {patch.logical_z_pauli_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Entanglement Boosting Circuit (Small Size)\n",
    "\n",
    "Overview of the entanglement boosting protocol:\n",
    "\n",
    "1. **Share physical Bell pairs**: Prepare noisy physical Bell pairs\n",
    "2. **Syndrome measurements on patch 1**: Error detection on the first patch\n",
    "3. **Syndrome measurements on patch 2**: Error detection on the second patch\n",
    "4. **Logical measurement**: Final logical Bell pair measurement\n",
    "\n",
    "We use small distances to keep it fast and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noiseless configuration (for easier understanding initially)\n",
    "noise_conf = util.NoiseConfiguration(\n",
    "    single_qubit_gate_error_probability=0.0,\n",
    "    two_qubit_gate_error_probability=0.0,\n",
    "    reset_error_probability=0.0,\n",
    "    measurement_error_probability=0.0,\n",
    "    idle_error_probability=0.0,\n",
    ")\n",
    "\n",
    "print(\"=== Building Entanglement Boosting Circuit ===\")\n",
    "print(f\"bell_distance (d_Bell) = 2\")\n",
    "print(f\"surface_distance (d_s) = 3\")\n",
    "print(f\"Bell error probability = 0.0\")\n",
    "print(f\"Post-selection = False\\n\")\n",
    "\n",
    "circuits = eb.perform_distillation(\n",
    "    bell_distance=2,\n",
    "    surface_distance=3,\n",
    "    noise_conf=noise_conf,\n",
    "    bell_error_probability=0.0,\n",
    "    post_selection=False,\n",
    ")\n",
    "\n",
    "print(\"Generated circuit (first 500 lines):\")\n",
    "circuit_str = str(circuits.circuit.circuit)\n",
    "lines = circuit_str.split('\\n')\n",
    "for line in lines[:500]:\n",
    "    print(line)\n",
    "if len(lines) > 500:\n",
    "    print(f\"\\n... (remaining {len(lines) - 500} lines)\")\n",
    "\n",
    "print(f\"\\nCircuit statistics:\")\n",
    "print(f\"  Number of qubits: {circuits.circuit.circuit.num_qubits}\")\n",
    "print(f\"  Number of measurements: {circuits.circuit.circuit.num_measurements}\")\n",
    "print(f\"  Number of detectors: {circuits.circuit.circuit.num_detectors}\")\n",
    "print(f\"  Number of observables: {circuits.circuit.circuit.num_observables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simulating a Few Shots\n",
    "\n",
    "Run the circuit a few times to see how results are collected. For each shot:\n",
    "1. Collect detection events (syndrome)\n",
    "2. Check post-selection conditions\n",
    "3. Calculate complementary gap\n",
    "4. Classify results into buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation with a small number of shots\n",
    "num_shots = 20\n",
    "results = eb.perform_simulation(circuits, num_shots=num_shots, seed=0)\n",
    "\n",
    "print(f\"=== Simulation Results ===\")\n",
    "print(f\"Total shots: {num_shots}\")\n",
    "print(f\"Discarded samples: {results.num_discarded_samples}\")\n",
    "print(f\"Valid samples: {len(results) - results.num_discarded_samples}\")\n",
    "print(f\"Maximum gap: {results.max_gap()}\")\n",
    "\n",
    "# Check contents of each bucket\n",
    "print(f\"\\n=== Gap Bucket Contents ===\")\n",
    "for gap, bucket in enumerate(results.buckets):\n",
    "    if len(bucket) > 0:\n",
    "        print(f\"Gap [{gap}, {gap+1}): valid={bucket.num_valid_samples}, wrong={bucket.num_wrong_samples}, total={len(bucket)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Inspection of Complementary Gap Buckets\n",
    "\n",
    "The complementary gap is an important metric in the entanglement boosting protocol. It represents the difference in minimum weight between different logical measurement outcomes.\n",
    "\n",
    "- **Large gap**: More reliable results\n",
    "- **Small gap**: Higher probability of errors\n",
    "\n",
    "Post-selection improves overall reliability by discarding results with small gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show non-empty buckets\n",
    "non_empty = [(i, b) for i, b in enumerate(results.buckets) if len(b) > 0]\n",
    "print(f\"Number of non-empty buckets: {len(non_empty)}\")\n",
    "print(\"\\nFirst 5 buckets:\")\n",
    "for gap, bucket in non_empty[:5]:\n",
    "    success_rate = bucket.num_valid_samples / len(bucket) if len(bucket) > 0 else 0\n",
    "    print(f\"  Gap [{gap}, {gap+1}): \"\n",
    "          f\"valid={bucket.num_valid_samples}, \"\n",
    "          f\"wrong={bucket.num_wrong_samples}, \"\n",
    "          f\"success_rate={success_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build circuit with noise configuration\n",
    "noise_conf_with_errors = util.NoiseConfiguration(\n",
    "    single_qubit_gate_error_probability=0.001,  # Single-qubit gate error probability\n",
    "    two_qubit_gate_error_probability=0.001,     # Two-qubit gate error probability\n",
    "    reset_error_probability=0.001,              # Reset error probability\n",
    "    measurement_error_probability=0.001,        # Measurement error probability\n",
    "    idle_error_probability=0.0001,              # Idle error probability\n",
    ")\n",
    "\n",
    "print(\"=== Building Circuit with Noise ===\")\n",
    "circuits_noisy = eb.perform_distillation(\n",
    "    bell_distance=2,\n",
    "    surface_distance=3,\n",
    "    noise_conf=noise_conf_with_errors,\n",
    "    bell_error_probability=0.01,  # Bell pair error probability (typically higher)\n",
    "    post_selection=False,\n",
    ")\n",
    "\n",
    "# Simulate with more shots\n",
    "print(\"\\n=== Simulation with Noise ===\")\n",
    "results_noisy = eb.perform_simulation(circuits_noisy, num_shots=1000, seed=42)\n",
    "\n",
    "print(f\"Total shots: 1000\")\n",
    "print(f\"Discarded samples: {results_noisy.num_discarded_samples}\")\n",
    "print(f\"Valid samples: {len(results_noisy) - results_noisy.num_discarded_samples}\")\n",
    "\n",
    "# Calculate error rate\n",
    "total_valid = sum(b.num_valid_samples for b in results_noisy.buckets)\n",
    "total_wrong = sum(b.num_wrong_samples for b in results_noisy.buckets)\n",
    "if total_valid + total_wrong > 0:\n",
    "    error_rate = total_wrong / (total_valid + total_wrong)\n",
    "    print(f\"Error rate: {error_rate:.4f} ({error_rate*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae7c3e",
   "metadata": {},
   "source": [
    "## 9. Simulation with Post-Selection\n",
    "\n",
    "Enabling post-selection allows discarding low-quality results through early error detection, improving the reliability of the remaining results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build circuit with post-selection enabled\n",
    "print(\"=== Building Circuit with Post-Selection ===\")\n",
    "circuits_postselect = eb.perform_distillation(\n",
    "    bell_distance=2,\n",
    "    surface_distance=3,\n",
    "    noise_conf=noise_conf_with_errors,\n",
    "    bell_error_probability=0.01,\n",
    "    post_selection=True,  # Enable post-selection\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "print(\"\\n=== Simulation with Post-Selection ===\")\n",
    "results_postselect = eb.perform_simulation(circuits_postselect, num_shots=1000, seed=42)\n",
    "\n",
    "print(f\"Total shots: 1000\")\n",
    "print(f\"Discarded samples: {results_postselect.num_discarded_samples}\")\n",
    "print(f\"Valid samples: {len(results_postselect) - results_postselect.num_discarded_samples}\")\n",
    "\n",
    "# Calculate error rate\n",
    "total_valid_ps = sum(b.num_valid_samples for b in results_postselect.buckets)\n",
    "total_wrong_ps = sum(b.num_wrong_samples for b in results_postselect.buckets)\n",
    "if total_valid_ps + total_wrong_ps > 0:\n",
    "    error_rate_ps = total_wrong_ps / (total_valid_ps + total_wrong_ps)\n",
    "    print(f\"Error rate: {error_rate_ps:.4f} ({error_rate_ps*100:.2f}%)\")\n",
    "    print(f\"\\nComparison with error rate without post-selection:\")\n",
    "    if total_valid + total_wrong > 0:\n",
    "        error_rate_no_ps = total_wrong / (total_valid + total_wrong)\n",
    "        print(f\"  Without post-selection: {error_rate_no_ps:.4f} ({error_rate_no_ps*100:.2f}%)\")\n",
    "        print(f\"  With post-selection: {error_rate_ps:.4f} ({error_rate_ps*100:.2f}%)\")\n",
    "        improvement = (error_rate_no_ps - error_rate_ps) / error_rate_no_ps * 100 if error_rate_no_ps > 0 else 0\n",
    "        print(f\"  Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30bcf7",
   "metadata": {},
   "source": [
    "## 10. Post-Selection Based on Complementary Gap\n",
    "\n",
    "Check error rates at different discard rates based on the complementary gap. Discarding results with small gaps can improve the reliability of the remaining results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e422dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error rates at different discard rates\n",
    "def analyze_discard_rates(results, num_shots):\n",
    "    \"\"\"Analyze error rates at different discard rates\"\"\"\n",
    "    discard_rates = [0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    num_unconditionally_discarded = results.num_discarded_samples\n",
    "    num_total_samples = len(results)\n",
    "    \n",
    "    print(f\"\\n=== Analysis by Discard Rate ===\")\n",
    "    print(f\"{'Discard Rate':<12} {'Gap Threshold':<15} {'Valid':<8} {'Wrong':<8} {'Discarded':<10} {'Error Rate':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for discard_rate in discard_rates:\n",
    "        num_valid = 0\n",
    "        num_wrong = 0\n",
    "        num_discarded = num_unconditionally_discarded\n",
    "        discarding = True\n",
    "        gap_threshold = results.max_gap()\n",
    "        \n",
    "        for (gap, bucket) in enumerate(results.buckets):\n",
    "            if discarding and len(bucket) + num_discarded <= num_total_samples * discard_rate:\n",
    "                num_discarded += len(bucket)\n",
    "                continue\n",
    "            \n",
    "            if discarding:\n",
    "                discarding = False\n",
    "                gap_threshold = gap\n",
    "            \n",
    "            num_valid += bucket.num_valid_samples\n",
    "            num_wrong += bucket.num_wrong_samples\n",
    "        \n",
    "        error_rate = num_wrong / (num_valid + num_wrong) if (num_valid + num_wrong) > 0 else float('nan')\n",
    "        print(f\"{discard_rate:<12.2f} {gap_threshold:<15} {num_valid:<8} {num_wrong:<8} {num_discarded:<10} {error_rate:<12.4f}\")\n",
    "\n",
    "# Analyze results with noise\n",
    "analyze_discard_rates(results_noisy, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3940df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of parallel simulation (using fewer shots for demonstration)\n",
    "print(\"=== Parallel Simulation Example ===\")\n",
    "print(\"Note: For demonstration, we use a small number of shots.\")\n",
    "print(\"In practice, use larger num_shots (e.g., 10000+) to see benefits of parallelism.\\n\")\n",
    "\n",
    "# Use the same circuit configuration as before\n",
    "parallel_results = eb.perform_parallel_simulation(\n",
    "    circuits_noisy,\n",
    "    num_shots=2000,\n",
    "    parallelism=2,  # Number of parallel processes\n",
    "    num_shots_per_task=1000,  # Shots per task\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Parallel Simulation Results ===\")\n",
    "print(f\"Total shots: 2000\")\n",
    "print(f\"Discarded samples: {parallel_results.num_discarded_samples}\")\n",
    "print(f\"Valid samples: {len(parallel_results) - parallel_results.num_discarded_samples}\")\n",
    "\n",
    "# Calculate error rate\n",
    "total_valid_par = sum(b.num_valid_samples for b in parallel_results.buckets)\n",
    "total_wrong_par = sum(b.num_wrong_samples for b in parallel_results.buckets)\n",
    "if total_valid_par + total_wrong_par > 0:\n",
    "    error_rate_par = total_wrong_par / (total_valid_par + total_wrong_par)\n",
    "    print(f\"Error rate: {error_rate_par:.4f} ({error_rate_par*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f70aa",
   "metadata": {},
   "source": [
    "## 12. Larger Distance Simulation\n",
    "\n",
    "Let's try a simulation with larger distances to see how the protocol scales. Note that larger distances provide better error correction but require more computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build circuit with larger distances\n",
    "print(\"=== Building Circuit with Larger Distances ===\")\n",
    "print(\"bell_distance (d_Bell) = 3\")\n",
    "print(\"surface_distance (d_s) = 5\")\n",
    "print(\"This will take longer to build and simulate...\\n\")\n",
    "\n",
    "circuits_large = eb.perform_distillation(\n",
    "    bell_distance=3,\n",
    "    surface_distance=5,\n",
    "    noise_conf=noise_conf_with_errors,\n",
    "    bell_error_probability=0.01,\n",
    "    post_selection=False,\n",
    ")\n",
    "\n",
    "print(f\"Circuit statistics:\")\n",
    "print(f\"  Number of qubits: {circuits_large.circuit.circuit.num_qubits}\")\n",
    "print(f\"  Number of measurements: {circuits_large.circuit.circuit.num_measurements}\")\n",
    "print(f\"  Number of detectors: {circuits_large.circuit.circuit.num_detectors}\")\n",
    "print(f\"  Number of observables: {circuits_large.circuit.circuit.num_observables}\")\n",
    "\n",
    "# Run a smaller number of shots for demonstration (larger distances are computationally expensive)\n",
    "print(\"\\n=== Simulation with Larger Distances ===\")\n",
    "print(\"Running 100 shots (larger distances require more computation)...\")\n",
    "results_large = eb.perform_simulation(circuits_large, num_shots=100, seed=123)\n",
    "\n",
    "print(f\"Total shots: 100\")\n",
    "print(f\"Discarded samples: {results_large.num_discarded_samples}\")\n",
    "print(f\"Valid samples: {len(results_large) - results_large.num_discarded_samples}\")\n",
    "\n",
    "# Calculate error rate\n",
    "total_valid_large = sum(b.num_valid_samples for b in results_large.buckets)\n",
    "total_wrong_large = sum(b.num_wrong_samples for b in results_large.buckets)\n",
    "if total_valid_large + total_wrong_large > 0:\n",
    "    error_rate_large = total_wrong_large / (total_valid_large + total_wrong_large)\n",
    "    print(f\"Error rate: {error_rate_large:.4f} ({error_rate_large*100:.2f}%)\")\n",
    "    print(f\"\\nComparison with smaller distance (d_s=3):\")\n",
    "    if total_valid + total_wrong > 0:\n",
    "        error_rate_small = total_wrong / (total_valid + total_wrong)\n",
    "        print(f\"  Small distance (d_s=3): {error_rate_small:.4f} ({error_rate_small*100:.2f}%)\")\n",
    "        print(f\"  Large distance (d_s=5): {error_rate_large:.4f} ({error_rate_large*100:.2f}%)\")\n",
    "        if error_rate_large < error_rate_small:\n",
    "            improvement = (error_rate_small - error_rate_large) / error_rate_small * 100\n",
    "            print(f\"  Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abcbd4c",
   "metadata": {},
   "source": [
    "## 13. Understanding the Command-Line Interface\n",
    "\n",
    "The repository also provides a command-line interface for running simulations. Let's see how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1abb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display help information for the command-line interface\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=== Command-Line Interface Usage ===\")\n",
    "print(\"\\nTo run simulations from the command line, use:\")\n",
    "print(\"  python src/entanglement_boosting.py [options]\\n\")\n",
    "print(\"Key options:\")\n",
    "print(\"  --num-shots: Number of simulation runs (default: 1000)\")\n",
    "print(\"  --error-probability: Error probability for gates (default: 0)\")\n",
    "print(\"  --bell-error-probability: Error probability for Bell pairs (default: 0)\")\n",
    "print(\"  --bell-distance: Distance of Bell pair in surface code\")\n",
    "print(\"  --surface-distance: Distance parameter for surface code patch (default: 3)\")\n",
    "print(\"  --post-selection: Enable early post-selection\")\n",
    "print(\"  --parallelism: Number of parallel processes (default: 1)\")\n",
    "print(\"  --show-progress: Show progress bar during simulation\")\n",
    "print(\"  --dump-results-to: Path to save simulation results (pickle format)\\n\")\n",
    "\n",
    "print(\"Example command:\")\n",
    "print(\"  python src/entanglement_boosting.py \\\\\")\n",
    "print(\"    --num-shots 10000 \\\\\")\n",
    "print(\"    --error-probability 0.001 \\\\\")\n",
    "print(\"    --bell-error-probability 0.01 \\\\\")\n",
    "print(\"    --surface-distance 5 \\\\\")\n",
    "print(\"    --bell-distance 3 \\\\\")\n",
    "print(\"    --parallelism 4 \\\\\")\n",
    "print(\"    --show-progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95ca2d",
   "metadata": {},
   "source": [
    "## 14. Visualizing Results\n",
    "\n",
    "Let's create some visualizations to better understand the simulation results. We'll plot the error rate as a function of discard rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01140b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import matplotlib for visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    discard_rates = [0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    error_rates = []\n",
    "    valid_counts = []\n",
    "    discarded_counts = []\n",
    "    \n",
    "    num_unconditionally_discarded = results_noisy.num_discarded_samples\n",
    "    num_total_samples = len(results_noisy)\n",
    "    \n",
    "    for discard_rate in discard_rates:\n",
    "        num_valid = 0\n",
    "        num_wrong = 0\n",
    "        num_discarded = num_unconditionally_discarded\n",
    "        discarding = True\n",
    "        \n",
    "        for (gap, bucket) in enumerate(results_noisy.buckets):\n",
    "            if discarding and len(bucket) + num_discarded <= num_total_samples * discard_rate:\n",
    "                num_discarded += len(bucket)\n",
    "                continue\n",
    "            \n",
    "            if discarding:\n",
    "                discarding = False\n",
    "            \n",
    "            num_valid += bucket.num_valid_samples\n",
    "            num_wrong += bucket.num_wrong_samples\n",
    "        \n",
    "        error_rate = num_wrong / (num_valid + num_wrong) if (num_valid + num_wrong) > 0 else float('nan')\n",
    "        error_rates.append(error_rate)\n",
    "        valid_counts.append(num_valid)\n",
    "        discarded_counts.append(num_discarded)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Error rate vs discard rate\n",
    "    ax1.plot(discard_rates, error_rates, 'o-', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Discard Rate', fontsize=12)\n",
    "    ax1.set_ylabel('Error Rate', fontsize=12)\n",
    "    ax1.set_title('Error Rate vs Discard Rate', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-0.05, 0.55)\n",
    "    \n",
    "    # Plot 2: Valid samples vs discard rate\n",
    "    ax2.plot(discard_rates, valid_counts, 's-', color='green', linewidth=2, markersize=8, label='Valid')\n",
    "    ax2.plot(discard_rates, discarded_counts, '^-', color='red', linewidth=2, markersize=8, label='Discarded')\n",
    "    ax2.set_xlabel('Discard Rate', fontsize=12)\n",
    "    ax2.set_ylabel('Number of Samples', fontsize=12)\n",
    "    ax2.set_title('Sample Distribution vs Discard Rate', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(-0.05, 0.55)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualization created successfully!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available. Skipping visualization.\")\n",
    "    print(\"To install: pip install matplotlib\")\n",
    "    print(\"\\nSummary of discard rate analysis:\")\n",
    "    analyze_discard_rates(results_noisy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a8a9b",
   "metadata": {},
   "source": [
    "## 15. Summary and Key Takeaways\n",
    "\n",
    "This notebook has walked through the entire entanglement boosting repository. Here's a summary of what we've learned:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80e99c",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "1. **Stim Library**: High-performance quantum circuit simulation library used for building and simulating quantum circuits.\n",
    "\n",
    "2. **Circuit Wrapper (`util.Circuit`)**: \n",
    "   - Enforces nearest-neighbor connectivity for two-qubit gates\n",
    "   - Manages one gate per tick per qubit\n",
    "   - Automatically injects noise based on gate types\n",
    "\n",
    "3. **Surface Code Patches**:\n",
    "   - Two patches are used in the entanglement boosting protocol\n",
    "   - Each patch performs syndrome measurements to detect errors\n",
    "   - Logical measurements are performed at the end\n",
    "\n",
    "4. **Entanglement Boosting Protocol**:\n",
    "   - Starts with noisy physical Bell pairs\n",
    "   - Uses small code (d_Bell) for initial error detection\n",
    "   - Escalates to larger code (d_s) for better error correction\n",
    "   - Performs soft-output decoding and post-selection\n",
    "\n",
    "5. **Complementary Gap**:\n",
    "   - Important metric for assessing result reliability\n",
    "   - Larger gaps indicate more reliable results\n",
    "   - Used for post-selection to improve overall fidelity\n",
    "\n",
    "6. **Post-Selection**:\n",
    "   - Early error detection allows discarding low-quality results\n",
    "   - Improves reliability of remaining results\n",
    "   - Can be based on complementary gap thresholds\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "- **Small distances** (d_s=3, d_Bell=2): Fast simulation, good for understanding\n",
    "- **Larger distances** (d_s=5+): Better error correction but computationally expensive\n",
    "- **Parallel simulation**: Use `perform_parallel_simulation` for large-scale runs\n",
    "- **Post-selection**: Improves error rates but reduces the number of valid samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3143ba",
   "metadata": {},
   "source": [
    "## 16. Next Steps and Further Exploration\n",
    "\n",
    "### Suggested Experiments\n",
    "\n",
    "1. **Vary noise parameters**: Experiment with different error probabilities to see how the protocol behaves under various noise conditions.\n",
    "\n",
    "2. **Explore different distances**: Try different combinations of `bell_distance` and `surface_distance` to find optimal trade-offs between error correction and computational cost.\n",
    "\n",
    "3. **Post-selection strategies**: Experiment with different discard rate thresholds to optimize the balance between error rate and sample efficiency.\n",
    "\n",
    "4. **Large-scale simulations**: Run simulations with thousands or tens of thousands of shots to get better statistics.\n",
    "\n",
    "5. **Compare with and without post-selection**: Systematically compare error rates with and without post-selection for different noise levels.\n",
    "\n",
    "### Code Structure\n",
    "\n",
    "The repository is organized as follows:\n",
    "\n",
    "- `src/entanglement_boosting.py`: Main implementation of the entanglement boosting protocol\n",
    "- `src/surface_code.py`: Surface code syndrome measurement implementations\n",
    "- `src/util.py`: Utility classes for circuit construction and noise management\n",
    "- `src/*_test.py`: Unit tests for various components\n",
    "\n",
    "### Running Tests\n",
    "\n",
    "To verify the implementation, you can run the unit tests:\n",
    "\n",
    "```bash\n",
    "python3 -m unittest *_test.py\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "For more details about the entanglement boosting protocol, see the paper:\n",
    "- \"Entanglement boosting: low-volume logical Bell pair preparation for large-scale fault-tolerant quantum computation\"\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This notebook has provided a comprehensive walkthrough of the entanglement boosting repository. You should now understand:\n",
    "- How to build and simulate entanglement boosting circuits\n",
    "- How noise affects the protocol\n",
    "- How post-selection improves reliability\n",
    "- How to use the command-line interface\n",
    "- How to analyze and visualize results\n",
    "\n",
    "Feel free to experiment with different parameters and explore the codebase further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Parallel Simulation for Large-Scale Runs\n",
    "\n",
    "For larger simulations, we can use parallel processing to speed up computation. The `perform_parallel_simulation` function distributes shots across multiple processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
